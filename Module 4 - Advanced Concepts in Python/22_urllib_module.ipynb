{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The urllib Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib module in Python\n",
    "\n",
    "urllib is a collection of modules for working with URLs. urllib includes the following modules:\n",
    "* urllib.request\n",
    "* urllib.error\n",
    "* urllib.parse\n",
    "* urllib.robotparser\n",
    "\n",
    "This chapter covers all of the above outside of urllib.error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/\n",
      "\n",
      "\n",
      "Date: Sat, 27 Nov 2021 17:59:17 GMT\n",
      "Expires: -1\n",
      "Cache-Control: private, max-age=0\n",
      "Content-Type: text/html; charset=ISO-8859-1\n",
      "P3P: CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"\n",
      "Server: gws\n",
      "X-XSS-Protection: 0\n",
      "X-Frame-Options: SAMEORIGIN\n",
      "Set-Cookie: 1P_JAR=2021-11-27-17; expires=Mon, 27-Dec-2021 17:59:17 GMT; path=/; domain=.google.com; Secure\n",
      "Set-Cookie: NID=511=Bf0UpUog0OYuE4teINawJvF1X5XM-p16QfxaQHl5hsnvqFm8-kIUFq8a-wsdO3VsEnwPacKl0nazdjQPkT9gWr6v7HZYDjmsOZ6eGDDbm7Yn1gKqUDJh2evvX3UdBa1SM4wtUs3cboEjHh5vMaTYipwILHTHL09TetspGrvg4lw; expires=Sun, 29-May-2022 17:59:17 GMT; path=/; domain=.google.com; HttpOnly\n",
      "Alt-Svc: h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\n",
      "Accept-Ranges: none\n",
      "Vary: Accept-Encoding\n",
      "Connection: close\n",
      "Transfer-Encoding: chunked\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Using the .urlopen function to get data\n",
    "import urllib\n",
    "\n",
    "url = urllib.request.urlopen('https://www.google.com/')\n",
    "\n",
    "# print some data associated with the url'\n",
    "# printing geturl is good for knowing if we called a redirect page\n",
    "print(url.geturl())\n",
    "print('\\n')\n",
    "\n",
    "# Info will give us some basic info on the page\n",
    "print(url.info())\n",
    "print('\\n')\n",
    "\n",
    "# Get code will provide us with the http response code\n",
    "print(url.getcode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miesner.jacob/python-for-programmers-educative/Module 4 - Advanced Concepts in Python/22_urllib_demos\n"
     ]
    }
   ],
   "source": [
    "# Downloading a file\n",
    "url = 'http://www.blog.pythonlibrary.org/wp-content/uploads/2012/06/wxDbViewer.zip'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read()\n",
    "\n",
    "%cd 22_urllib_demos\n",
    "\n",
    "with open('test.zip', 'wb') as fobj:\n",
    "    fobj.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you visit a website with your browser, the browser tells the website who it is. This is called the user-agent string. Some websites won’t recognize this user-agent string and will behave in strange ways or not work at all. Fortunately, it’s easy for you to set up your own custom user-agent string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying User Agent\n",
    "\n",
    "user_agent = ' Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0'\n",
    "url = 'http://www.whatsmyua.com/'\n",
    "headers = {'User-Agent': user_agent}\n",
    "request = urllib.request.Request(url, headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    with open('user_agent.html', 'wb') as out:\n",
    "        out.write(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full result: ParseResult(scheme='https', netloc='duckduckgo.com', path='/', params='', query='q=python+stubbing&t=canonical&ia=qa', fragment='')\n",
      "\n",
      "Netloc: duckduckgo.com\n",
      "\n",
      "Get url: https://duckduckgo.com/?q=python+stubbing&t=canonical&ia=qa\n",
      "\n",
      "Port: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parsing a url\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "result = urlparse('https://duckduckgo.com/?q=python+stubbing&t=canonical&ia=qa')\n",
    "\n",
    "# Printing out some data we can get for urllib.parse\n",
    "print(\"Full result: \" + str(result) + '\\n')\n",
    "print(\"Netloc: \" + str(result.netloc) + '\\n')\n",
    "print(\"Get url: \" + str(result.geturl()) + '\\n')\n",
    "print(\"Port: \" + str(result.port) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q=Python\n"
     ]
    }
   ],
   "source": [
    "# Submitting a Web Form using the urlencode method\n",
    "\n",
    "# Encode data\n",
    "data = urllib.parse.urlencode({'q': 'Python'})\n",
    "print(data)\n",
    "\n",
    "# Submitting query to duckduckgo and saving result\n",
    "url = 'http://duckduckgo.com/html/'\n",
    "full_url = url + '?' + data\n",
    "response = urllib.request.urlopen(full_url)\n",
    "with open('results.html', 'wb') as f:\n",
    "    f.write(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib.robotparser\n",
    "\n",
    "The robotparser module is made up of a single class, RobotFileParser. This class will answer questions about whether or not a specific user agent can fetch a URL that has a published robot.txt file. The robots.txt file will tell a web scraper or robot what parts of the server should not be accessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arstechnica.com/\n",
      "True\n",
      "\n",
      "\n",
      "http://arstechnica.com/cgi-bin/\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "\n",
    "# Instantiate RobotFileParser class and set url\n",
    "robot = urllib.robotparser.RobotFileParser()\n",
    "robot.set_url('http://arstechnica.com/robots.txt')\n",
    "robot.read()\n",
    "\n",
    "# Can we crawl this page?\n",
    "page = 'http://arstechnica.com/'\n",
    "print(page)\n",
    "print(robot.can_fetch('*', page))\n",
    "print('\\n')\n",
    "\n",
    "# Can we crawl this page?\n",
    "page = 'http://arstechnica.com/cgi-bin/'\n",
    "print(page)\n",
    "print(robot.can_fetch('*', page))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
