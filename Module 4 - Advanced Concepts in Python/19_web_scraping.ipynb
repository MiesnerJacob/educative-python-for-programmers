{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Web scraping is where you write an application to download web pages and parse out information from them.\n",
    "\n",
    "Scraping tips:\n",
    "* Check website terms & conditions, there is usually a limit on how often and what you can scrape.\n",
    "* Don't hammer the website with requests...\n",
    "* You can get into legal. trouble if you don't follow the tips above\n",
    "* Websites change so if you want your scraper to work over time you will have to maintain it\n",
    "* Website data can be messy, you will have to clean it most times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Scrape\n",
    "\n",
    "You can use Python’s urllib2 module to download the HTML that we need to parse or you can use the requests library. For this example, we will use requests.\n",
    "\n",
    "Using the inspect option in your browser to view the webpage HTML can be extremelyt useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "A popular HTML parser for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from beautifulsoup4) (2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install BeautifulSoup\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping using BeatifulSoup to get article headers\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://www.blog.pythonlibrary.org/'\n",
    "\n",
    "def get_articles():\n",
    "    \"\"\"\n",
    "    Get the articles from the front page of the blog\n",
    "    \"\"\"\n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    pages = soup.findAll('h1')\n",
    "\n",
    "    articles = {i.a['href']: i.text.strip()\n",
    "                for i in pages if i.a}\n",
    "    \n",
    "    for article in articles:\n",
    "        s = '{title}: {url}'.format(title=articles[article].encode('utf-8'),url=article)\n",
    "        print(s)\n",
    "\n",
    "    return articles\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    articles = get_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping twitter using requests in conjuction with BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://twitter.com/mousevspython'\n",
    "req = requests.get(url)\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tweets = soup.findAll('li', 'js-stream-item')\n",
    "for item in range(len(soup.find_all('p', 'TweetTextSize'))):\n",
    "    tweet_text = tweets[item].get_text().encode('utf-8')\n",
    "    print(tweet_text)\n",
    "    dt = tweets[item].find('a', 'tweet-timestamp')\n",
    "    print('This was tweeted on ' + str(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy\n",
    "\n",
    "Scrapy is a framework that you can use for crawling websites and extracting (i.e. scraping) data. It can also be used to extract data via a website’s API or as a general purpose web crawler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (0.2.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cryptography>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (36.0.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (20.3.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (5.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/lxml-4.6.1-py3.8-macosx-10.9-x86_64.egg (from scrapy) (4.6.1)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: six>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/attrs-20.2.0-py3.8.egg (from service-identity>=16.0.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: pyasn1-modules in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyasn1_modules-0.2.8-py3.8.egg (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyasn1-0.4.8-py3.8.egg (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy) (49.2.1)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n",
      "Requirement already satisfied: idna>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n"
     ]
    }
   ],
   "source": [
    "# Install Scrapy\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'blog_scraper', using template directory '/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scrapy/templates/project', created in:\n",
      "    /Users/miesner.jacob/python-for-programmers-educative/Module 4 - Advanced Concepts in Python/blog_scraper\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd blog_scraper\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject blog_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we changed our items.py to define what we want to scrape, added a blog.py file to create our spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miesner.jacob/python-for-programmers-educative/Module 4 - Advanced Concepts in Python/blog_scraper\n"
     ]
    }
   ],
   "source": [
    "# Navigate to blog scraper folder\n",
    "%cd blog_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-26 16:30:55 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: blog_scraper)\n",
      "2021-11-26 16:30:55 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.0, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2021-11-26 16:30:55 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-11-26 16:30:55 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'blog_scraper',\n",
      " 'NEWSPIDER_MODULE': 'blog_scraper.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['blog_scraper.spiders']}\n",
      "2021-11-26 16:30:55 [scrapy.extensions.telnet] INFO: Telnet Password: 79ce4922fddddda2\n",
      "2021-11-26 16:30:55 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-11-26 16:30:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-11-26 16:30:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-11-26 16:30:55 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-11-26 16:30:55 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-11-26 16:30:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-11-26 16:30:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-11-26 16:30:55 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://blog.pythonlibrary.org/robots.txt> from <GET http://blog.pythonlibrary.org/robots.txt>\n",
      "2021-11-26 16:30:56 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.blog.pythonlibrary.org/robots.txt> from <GET https://blog.pythonlibrary.org/robots.txt>\n",
      "2021-11-26 16:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.blog.pythonlibrary.org/robots.txt> (referer: None)\n",
      "2021-11-26 16:30:57 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://blog.pythonlibrary.org/> from <GET http://blog.pythonlibrary.org>\n",
      "2021-11-26 16:30:59 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.blog.pythonlibrary.org/> from <GET https://blog.pythonlibrary.org/>\n",
      "2021-11-26 16:31:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.blog.pythonlibrary.org/robots.txt> (referer: None)\n",
      "2021-11-26 16:31:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.blog.pythonlibrary.org/> (referer: None)\n",
      "2021-11-26 16:31:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-11-26 16:31:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1606,\n",
      " 'downloader/request_count': 7,\n",
      " 'downloader/request_method_count/GET': 7,\n",
      " 'downloader/response_bytes': 20505,\n",
      " 'downloader/response_count': 7,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'downloader/response_status_count/301': 4,\n",
      " 'elapsed_time_seconds': 5.466348,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 11, 26, 21, 31, 0, 692555),\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 53981184,\n",
      " 'memusage/startup': 53981184,\n",
      " 'response_received_count': 3,\n",
      " 'robotstxt/request_count': 2,\n",
      " 'robotstxt/response_count': 2,\n",
      " 'robotstxt/response_status_count/200': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2021, 11, 26, 21, 30, 55, 226207)}\n",
      "2021-11-26 16:31:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Run our crawler\n",
    "!scrapy crawl mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scrapy/commands/__init__.py:131: ScrapyDeprecationWarning: ('The -t command line option is deprecated in favor of specifying the output format within the output URI. See the documentation of the -o and -O options for more information.',)\n",
      "  feeds = feed_process_params_from_cli(\n",
      "2021-11-26 16:31:01 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: blog_scraper)\n",
      "2021-11-26 16:31:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39) - [Clang 6.0 (clang-600.0.57)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.0, Platform macOS-10.14.6-x86_64-i386-64bit\n",
      "2021-11-26 16:31:01 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-11-26 16:31:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'blog_scraper',\n",
      " 'NEWSPIDER_MODULE': 'blog_scraper.spiders',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['blog_scraper.spiders']}\n",
      "2021-11-26 16:31:01 [scrapy.extensions.telnet] INFO: Telnet Password: b050cc5382edd303\n",
      "2021-11-26 16:31:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-11-26 16:31:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-11-26 16:31:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-11-26 16:31:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-11-26 16:31:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-11-26 16:31:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-11-26 16:31:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-11-26 16:31:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://blog.pythonlibrary.org/robots.txt> from <GET http://blog.pythonlibrary.org/robots.txt>\n",
      "2021-11-26 16:31:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.blog.pythonlibrary.org/robots.txt> from <GET https://blog.pythonlibrary.org/robots.txt>\n",
      "2021-11-26 16:31:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.blog.pythonlibrary.org/robots.txt> (referer: None)\n",
      "2021-11-26 16:31:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://blog.pythonlibrary.org/> from <GET http://blog.pythonlibrary.org>\n",
      "2021-11-26 16:31:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.blog.pythonlibrary.org/> from <GET https://blog.pythonlibrary.org/>\n",
      "2021-11-26 16:31:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.blog.pythonlibrary.org/robots.txt> (referer: None)\n",
      "2021-11-26 16:31:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.blog.pythonlibrary.org/> (referer: None)\n",
      "2021-11-26 16:31:07 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-11-26 16:31:07 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1606,\n",
      " 'downloader/request_count': 7,\n",
      " 'downloader/request_method_count/GET': 7,\n",
      " 'downloader/response_bytes': 20505,\n",
      " 'downloader/response_count': 7,\n",
      " 'downloader/response_status_count/200': 3,\n",
      " 'downloader/response_status_count/301': 4,\n",
      " 'elapsed_time_seconds': 5.833322,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 11, 26, 21, 31, 7, 539585),\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 54550528,\n",
      " 'memusage/startup': 54546432,\n",
      " 'response_received_count': 3,\n",
      " 'robotstxt/request_count': 2,\n",
      " 'robotstxt/response_count': 2,\n",
      " 'robotstxt/response_status_count/200': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2021, 11, 26, 21, 31, 1, 706263)}\n",
      "2021-11-26 16:31:07 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "# Run our crawler and export to csv\n",
    "!scrapy crawl mouse -o articles.csv -t csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
